# -*- coding: utf-8 -*-
"""malaria_multicell_class.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/195-_OiTzUrK0XxEdecxm0mdVFRzpGqTu
"""

from google.colab import drive
drive.mount("/content/gdrive", force_remount=True)
#drive.mount('/content/gdrive/')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import tensorflow as tf

train_ds = tf.keras.utils.image_dataset_from_directory(
    directory='/content/gdrive/MyDrive/img2/',
    validation_split=0.2,
    subset='training',
    shuffle = True,
    seed=111,
    image_size=(224, 224),
    batch_size=64)

val_ds = tf.keras.utils.image_dataset_from_directory(
    directory='/content/gdrive/MyDrive/img2/',
    validation_split=0.2,
    subset="validation",
    shuffle = True,
    seed=111,
    image_size=(224, 224),
    batch_size=64)

class_names = train_ds.class_names
print(class_names)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

from keras import layers

normalization_layer = layers.Rescaling(1./255)

normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized_ds))
first_image = image_batch[0]
# Notice the pixel values are now in `[0,1]`.
print(np.min(first_image), np.max(first_image))

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D,MaxPool2D,Dropout,Flatten,Dense,BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam

model = Sequential()
model.add(Conv2D(16,(3,3),activation='relu',input_shape=(224,224,3)))
model.add(MaxPool2D(2,2))
model.add(Dropout(0.2))

model.add(Conv2D(32,(3,3),activation='relu'))
model.add(MaxPool2D(2,2))
model.add(Dropout(0.2))

model.add(Conv2D(64,(3,3),activation='relu'))
model.add(MaxPool2D(2,2))
model.add(Dropout(0.2))

model.add(Flatten())

model.add(Dense(1,activation='sigmoid'))

early_stopping = EarlyStopping(monitor='val_loss', patience=5)

model.compile(optimizer = Adam(learning_rate = 0.001), loss = 'binary_crossentropy', metrics = ['accuracy'])

model.summary()

epochs = 20
history = model.fit(train_ds, validation_data = val_ds, epochs = epochs,  callbacks = [early_stopping])

model.save("malaria_class.h5")
np.save("malaria_class.npy", history)

import os
import tensorflow as tf
import numpy as np
from PIL import Image

# Load the dataset
train_ds = tf.keras.utils.image_dataset_from_directory(
    directory='/content/gdrive/MyDrive/img1/',
    validation_split=0.2,
    subset='validation',
    shuffle=True,
    seed=111,
    image_size=(256, 256),
    batch_size=64)

# Get class names
class_names = train_ds.class_names

# Create output directory
output_dir = '/content/gdrive/MyDrive/test_images'
os.makedirs(output_dir, exist_ok=True)

# Create a subdirectory for each class
for class_name in class_names:
    class_dir = os.path.join(output_dir, class_name)
    os.makedirs(class_dir, exist_ok=True)

# Function to save images to directories
def save_images_from_dataset(dataset, output_dir, class_names):
    for images, labels in dataset:
        for i in range(images.shape[0]):
            image = images[i].numpy().astype(np.uint8)
            label = labels[i].numpy()
            class_name = class_names[label]
            image_pil = Image.fromarray(image)
            file_name = f'{class_name}_{i}.jpg'
            file_path = os.path.join(output_dir, class_name, file_name)
            image_pil.save(file_path)

# Save images
save_images_from_dataset(train_ds, output_dir, class_names)

print('Images saved successfully.')

import os
import shutil
from sklearn.model_selection import train_test_split

# Define the source and destination directories
source_dir = '/content/gdrive/MyDrive/img2/'
train_dir = '/content/gdrive/MyDrive/dataset/train/'
test_dir = '/content/gdrive/MyDrive/dataset/test/'

# Create train and test directories
os.makedirs(train_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

# Create subdirectories for each class in the train and test directories
class_names = os.listdir(source_dir)
for class_name in class_names:
    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)
    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)

# Function to split and save images
def split_and_save_images(source_dir, train_dir, test_dir, class_names, test_size=0.2, seed=111):
    for class_name in class_names:
        class_path = os.path.join(source_dir, class_name)
        images = os.listdir(class_path)

        train_images, test_images = train_test_split(images, test_size=test_size, random_state=seed)

        # Copy images to respective directories
        for image in train_images:
            shutil.copy(os.path.join(class_path, image), os.path.join(train_dir, class_name, image))

        for image in test_images:
            shutil.copy(os.path.join(class_path, image), os.path.join(test_dir, class_name, image))

# Split and save the images
split_and_save_images(source_dir, train_dir, test_dir, class_names)

print("Images split and saved successfully.")

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, accuracy_score  # Import accuracy_score here

# Data augmentation for training set
train_datagen = ImageDataGenerator(
    rescale=1.0/255.0,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Simple rescaling for validation/test set
test_datagen = ImageDataGenerator(rescale=1.0/255.0)

# Directory paths
train_dir = '/content/gdrive/MyDrive/train_images'
test_dir = '/content/gdrive/MyDrive/test_images'

# Training and test generators
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

# Model architecture (example)
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(2, activation='softmax')
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(
    train_generator,
    epochs=30,
    validation_data=test_generator,
    callbacks=[early_stopping]
)

# Save the model
model.save('/content/malaria_class_augmented.h5')

# Evaluate on test data
predictions = model.predict(test_generator, steps=np.ceil(test_generator.n / test_generator.batch_size))
predicted_classes = np.argmax(predictions, axis=1)
true_classes = test_generator.classes
class_labels = list(test_generator.class_indices.keys())

report = classification_report(true_classes, predicted_classes, target_names=class_labels)
print(report)

accuracy = accuracy_score(true_classes, predicted_classes)
print(f'Accuracy: {accuracy * 100:.2f}%')

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam

# Data augmentation for training set
train_datagen = ImageDataGenerator(
    rescale=1.0/255.0,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Simple rescaling for validation/test set
test_datagen = ImageDataGenerator(rescale=1.0/255.0)

# Directory paths
train_dir = '/content/gdrive/MyDrive/dataset/train'
test_dir = '/content/gdrive/MyDrive/dataset/test'

# Training and test generators
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

# Model architecture (example)
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(2, activation='softmax')
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(
    train_generator,
    epochs=30,
    validation_data=test_generator,
    callbacks=[early_stopping]
)

# Save the model
model.save('/content/malaria_class_augmented.h5')

# Evaluate on test data
predictions = model.predict(test_generator, steps=np.ceil(test_generator.n / test_generator.batch_size))
predicted_classes = np.argmax(predictions, axis=1)
true_classes = test_generator.classes
class_labels = list(test_generator.class_indices.keys())

report = classification_report(true_classes, predicted_classes, target_names=class_labels)
print(report)

accuracy = accuracy_score(true_classes, predicted_classes)
print(f'Accuracy: {accuracy * 100:.2f}%')

from sklearn.metrics import classification_report, accuracy_score  # Import accuracy_score here
accuracy = accuracy_score(true_classes, predicted_classes)
print(f'Accuracy: {accuracy * 100:.2f}%')

import tensorflow as tf
import matplotlib.pyplot as plt

# Define your training directory
train_dir = '/content/gdrive/MyDrive/dataset/train/'

# Create a TensorFlow dataset from the directory
train_ds = tf.keras.utils.image_dataset_from_directory(
    train_dir,
    image_size=(224, 224),
    batch_size=64,
    label_mode='int'  # Use 'int' for integer labels
)

# Get the class names (folders in the directory)
class_names = train_ds.class_names

# Function to display images with labels
def display_images_with_labels(dataset, class_names):
    for images, labels in dataset.take(1):  # Take 1 batch
        plt.figure(figsize=(15, 15))
        for i in range(9):
            ax = plt.subplot(8, 8, i + 1)  # Assuming batch size of 64 (8x8 grid)
            plt.imshow(images[i].numpy().astype("uint8"))
            plt.title(class_names[labels[i]])
            plt.axis("off")
        plt.show()
        break  # Only display one batch

# Display images with labels
display_images_with_labels(train_ds, class_names)

import os
print("Current directory:", os.getcwd())
print("Files in current directory:", os.listdir())

!pip install tensorflow

import tensorflow as tf

# Load the Keras model from .h5 file
model = tf.keras.models.load_model("malaria_class_augmented.h5")

# Convert the model to TensorFlow Lite format
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the TensorFlow Lite model to a .tflite file
with open("malaria_classificationbal.tflite", "wb") as f:
    f.write(tflite_model)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(1, len(history.history['accuracy']) + 1)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

import tensorflow as tf
import numpy as np
from PIL import Image
import os
import matplotlib.pyplot as plt

# Load the TFLite model
interpreter = tf.lite.Interpreter(model_path="/content/gdrive/MyDrive/malaria_classificationbal.tflite")
interpreter.allocate_tensors()

# Get input and output details
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Get input shape
input_shape = input_details[0]['shape']
IMG_HEIGHT, IMG_WIDTH = input_shape[1], input_shape[2]

def preprocess_image(image_path):
    # Load image
    img = Image.open(image_path).convert('RGB')
    # Resize image
    img = img.resize((IMG_WIDTH, IMG_HEIGHT))
    # Convert image to numpy array
    img = np.array(img).astype(np.float32)  # Ensure the data type is float32
    # Normalize the image to [0, 1]
    img = img / 255.0
    # Add batch dimension
    img = np.expand_dims(img, axis=0)
    return img

def predict_image(image_path):
    # Preprocess the image
    input_data = preprocess_image(image_path)

    # Set the tensor to point to the input data to be inferred
    interpreter.set_tensor(input_details[0]['index'], input_data)

    # Run the inference
    interpreter.invoke()

    # Get the results
    output_data = interpreter.get_tensor(output_details[0]['index'])
    return output_data

# Define class labels
class_labels = ['infected', 'uninfected']

def predict_images_in_folder(folder_path):
    # Iterate through each image in the folder
    for filename in os.listdir(folder_path):
        if filename.endswith(".jpg") or filename.endswith(".png"):  # Add other image formats if needed
            image_path = os.path.join(folder_path, filename)
            predictions = predict_image(image_path)
            predicted_class = class_labels[np.argmax(predictions)]

            # Load the original image for visualization
            original_img = Image.open(image_path).convert('RGB')

            # Display the image with the predicted label
            plt.imshow(original_img)
            plt.title(f"Predicted Class: {predicted_class}")
            plt.axis('off')  # Hide axes
            plt.show()

# Example usage
predict_images_in_folder('/content/gdrive/MyDrive/testdata')

"""# converting text files of images into dictionary"""

import os
import re

def convert_line_to_list(line):
    """
    Convert a line of text to a list of integers.
    Handles both comma-separated and space-separated strings of numbers.
    """
    try:
        # Use a regular expression to split the line by commas and spaces
        return [int(x) for x in re.split(r'[,\s]+', line) if x]
    except ValueError as e:
        print(f"Error converting line to list: {line}. Error: {e}")
        return []

# Define the folder path containing the text files
folder_path = '/content/gdrive/MyDrive/malaria1/Label'

# Initialize an empty dictionary to store the data
data_dict = {}

# Iterate over each file in the folder
for filename in os.listdir(folder_path):
    # Check if the file is a text file
    if filename.endswith('.txt'):
        # Initialize an empty list to store the lines of the current text file
        lines_list = []

        # Construct the full file path
        file_path = os.path.join(folder_path, filename)

        # Open the file and read the contents
        with open(file_path, 'r') as file:
            for line in file:
                # Strip newline characters and any leading/trailing whitespace
                stripped_line = line.strip()
                # Convert the line to a list of integers
                line_list = convert_line_to_list(stripped_line)
                if line_list:  # Only add the line if it was successfully converted
                    lines_list.append(line_list)

        # Add the list of lines to the dictionary with the filename (without extension) as the key
        data_dict[os.path.splitext(filename)[0]] = lines_list

# Print the dictionary to verify the contents
for key, value in data_dict.items():
    print(f'{key}: {value}')

"""# calculating malaria percentage"""

import pandas as pd

# Example dictionary with image names and bounding box values
"""annotations_dict = {
    "image1.jpg": [
        [0, 9, 393, 134, 515],
        [0, 201, 1027, 329, 1146],
        [1, 96, 498, 213, 624],
        [1, 666, 708, 788, 823],
        # Add more bounding boxes for this image
    ],
    "image2.jpg": [
        [1, 70, 718, 209, 854],
        [0, 684, 346, 811, 477],
        [1, 25, 304, 143, 438],
        # Add more bounding boxes for this image
    ],
    # Add more images as needed
}"""

# Fixed image size (width, height)
image_width = 1600
image_height = 1200
total_image_area = image_width * image_height

# Collect all annotations in a list
annotations_data = []
for image_id, bboxes in data_dict.items():
    for bbox in bboxes:
        annotations_data.append([image_id] + bbox)

# Convert to DataFrame
annotations_df = pd.DataFrame(annotations_data, columns=["image_id", "label", "x1", "y1", "x2", "y2"])

# Calculate the area of each bounding box
def calculate_area(row):
    if row['label'] == 0:  # Label 0 means not affected by malaria
        return 0
    width = row['x2'] - row['x1']
    height = row['y2'] - row['y1']
    return width * height

annotations_df['affected_area'] = annotations_df.apply(calculate_area, axis=1)

# Calculate the affected percentage for each image
results = []
for image_id, group in annotations_df.groupby('image_id'):
    total_affected_area = group['affected_area'].sum()
    affected_percentage = (total_affected_area / total_image_area) * 100
    results.append({"image_id": image_id, "affected_percentage": affected_percentage})

results_df = pd.DataFrame(results)
print(results_df)

"""# converting to csv"""

results_df.to_csv('results.csv', index=False)

"""# regression model"""

import numpy as np
import pandas as pd
import cv2
import os
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from keras.applications.vgg16 import VGG16, preprocess_input
from keras.models import Model

# Load pre-trained VGG16 model + higher level layers
base_model = VGG16(weights='imagenet')
model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc1').output)

# Function to extract features using VGG16
def extract_features(image_path):
    img = cv2.imread(image_path)
    img = cv2.resize(img, (224, 224))
    img = preprocess_input(img)
    img = np.expand_dims(img, axis=0)
    features = model.predict(img)
    return features[0]

# Load CSV file with image IDs and affected malaria percentage
annotations_csv = '/content/results.csv'  # Replace with your CSV file path
df_annotations = pd.read_csv(annotations_csv)

# Directory containing images
images_dir = '/content/gdrive/MyDrive/malaria1/images'  # Replace with your directory path

# Prepare the dataset
data = []
for index, row in df_annotations.iterrows():
    image_id = row['image_id']  # Assuming 'image_id' column contains image filenames
    image_path = os.path.join(images_dir,f"{image_id}.png")

    if not os.path.exists(image_path):
        print(f"Image path does not exist: {image_path}")
        continue

    # Extract image features
    features = extract_features(image_path)

    # Append features and affected percentage to data list
    data.append(np.append(features, row['affected_percentage']))

# Convert to DataFrame
columns = [f'feature_{i}' for i in range(4096)] + ['affected_percentage']
df = pd.DataFrame(data, columns=columns)

# Split the data into features (X) and target (y)
X = df.drop('affected_percentage', axis=1)
y = df['affected_percentage']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a regression model
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# Predict on the test set
y_pred = regressor.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

import numpy as np
import pandas as pd
import cv2
import os
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from keras.applications.vgg16 import VGG16, preprocess_input
from keras.models import Model

# Load pre-trained VGG16 model + higher level layers
base_model = VGG16(weights='imagenet')
model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc1').output)

# Function to extract features using VGG16
def extract_features(image_path):
    img = cv2.imread(image_path)
    img = cv2.resize(img, (224, 224))
    img = preprocess_input(img)
    img = np.expand_dims(img, axis=0)
    features = model.predict(img)
    return features[0]

# Load CSV file with image IDs and affected malaria percentage
annotations_csv = '/content/gdrive/MyDrive/malaria1/results.csv'  # Replace with your CSV file path
df_annotations = pd.read_csv(annotations_csv)

# Directory containing images
images_dir = '/content/gdrive/MyDrive/malaria1/images'  # Replace with your directory path

# Prepare the dataset
data = []
for index, row in df_annotations.iterrows():
    image_id = row['image_id']  # Assuming 'image_id' column contains image filenames
    image_path = os.path.join(images_dir, f"{image_id}.png")

    if not os.path.exists(image_path):
        print(f"Image path does not exist: {image_path}")
        continue

    # Extract image features
    features = extract_features(image_path)

    # Append features and affected percentage to data list
    data.append(np.append(features, row['affected_percentage']))

# Convert to DataFrame
columns = [f'feature_{i}' for i in range(4096)] + ['affected_percentage']
df = pd.DataFrame(data, columns=columns)

# Split the data into features (X) and target (y)
X = df.drop('affected_percentage', axis=1)
y = df['affected_percentage']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a regression model
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# Predict on the test set
y_pred = regressor.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# Calculate R^2 score (coefficient of determination)
r2 = r2_score(y_test, y_pred)
print(f'R^2 Score: {r2}')